{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wObdGcJrxOHw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgSSNJ60yv4v",
    "outputId": "7f6e6a11-6fe5-455b-e418-ddb02c539515"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "num_gpus_available = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", num_gpus_available)\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UrAdHLbfy1WA"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"complete_preprocessing.csv\",encoding = \"utf-8\",sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LInkTieSYN00"
   },
   "outputs": [],
   "source": [
    "df_dev = pd.read_csv(\"dev_complete_preprocessing.csv\",encoding = \"utf-8\",sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Es1fpJGXa7AA"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"test_complete_preprocessing_stp_frsc.csv\",encoding = \"utf-8\",sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e8OB-any6A_"
   },
   "outputs": [],
   "source": [
    "df['note'] = df['note'].astype('int')\n",
    "df_dev['note'] = df_dev['note'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "itUek-6m2oJc",
    "outputId": "546cec43-1f69-4bff-c4a0-a1d92028f7fc"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "h-XIvpGmYbPP",
    "outputId": "6c30fa28-5696-4e6d-95f4-4af119fc6180"
   },
   "outputs": [],
   "source": [
    "df_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "JSfUNlika8xB",
    "outputId": "dd96ac1a-591b-4ec6-a73f-499e412b2998"
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7hHhplca7tLV"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df_dev = df_dev.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iudCgAqv7x8F"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['index'],inplace=True)\n",
    "df_dev.drop(columns=['index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9lFb6hR9rCX"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "def maxNumWords360(text):\n",
    "    l=text_to_word_sequence(str(text))\n",
    "    if len(l) >= 360:\n",
    "        l = l[:360]\n",
    "    return \" \".join(l)\n",
    "\n",
    "maxWordsNum = 0\n",
    "def maxNumWords(text):\n",
    "    global maxWordsNum\n",
    "    l = text_to_word_sequence(str(text))\n",
    "    if len(l) > maxWordsNum:\n",
    "        maxWordsNum = len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YdKxIwCB9tls",
    "outputId": "4c1d3bff-6c82-41c3-a66e-fadb25e177fc"
   },
   "outputs": [],
   "source": [
    "df.commentaire.apply(maxNumWords)\n",
    "print(\"Max num of words per sentence in the training set is: \",maxWordsNum)\n",
    "maxWordsNum = 0\n",
    "df_dev.commentaire.apply(maxNumWords)\n",
    "print(\"Max num of words per sentence in the dev set is: \",maxWordsNum)\n",
    "maxWordsNum = 0\n",
    "\n",
    "df.commentaire=df.commentaire.apply(maxNumWords360)\n",
    "df_dev.commentaire=df_dev.commentaire.apply(maxNumWords360)\n",
    "\n",
    "df.commentaire.apply(maxNumWords)\n",
    "print(\"Max num of words per sentence in the training set is: \",maxWordsNum)\n",
    "maxWordsNum = 0\n",
    "df_dev.commentaire.apply(maxNumWords)\n",
    "print(\"Max num of words per sentence in the dev set is: \",maxWordsNum)\n",
    "maxWordsNum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rzBdhyIZbFJS",
    "outputId": "5d54c117-ed11-4564-a63f-2c223ba09e2c"
   },
   "outputs": [],
   "source": [
    "df_test.commentaire.apply(maxNumWords)\n",
    "print(\"Max num of words per sentence in the training set is: \",maxWordsNum)\n",
    "maxWordsNum = 0\n",
    "\n",
    "df_test.commentaire=df_test.commentaire.apply(maxNumWords360)\n",
    "\n",
    "df_test.commentaire.apply(maxNumWords)\n",
    "print(\"Max num of words per sentence in the training set is: \",maxWordsNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAwqjtsm5L-Y"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from keras.layers import Dropout,BatchNormalization\n",
    "from keras import regularizers\n",
    "import gensim\n",
    "from gensim.models import FastText\n",
    "import nltk\n",
    "from nltk.tokenize import SpaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KUHHfyWea9zo"
   },
   "outputs": [],
   "source": [
    "X_train = df.commentaire.values\n",
    "y_train = df.note.values\n",
    "X_val = df_dev.commentaire.values\n",
    "y_val = df_dev.note.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "alFRRuKebscw"
   },
   "outputs": [],
   "source": [
    "X_test = df_test.commentaire.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78yGKRNX9Rk-"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3oOBMUM_I_C",
    "outputId": "7709e2c3-3865-4840-d6d6-bf07f0156729"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.index_word) + 1\n",
    "print('Vocab Size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzBxYwku-0mi"
   },
   "outputs": [],
   "source": [
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_token = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YODOdiRbvwN"
   },
   "outputs": [],
   "source": [
    "X_test_token = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_PYoUfXw-3Ev"
   },
   "outputs": [],
   "source": [
    "sequence_len = 360\n",
    "X_train_token = pad_sequences(X_train_token, padding='post', maxlen=sequence_len)\n",
    "X_val_token = pad_sequences(X_val_token, padding='post', maxlen=sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34S6cvgOUUTx"
   },
   "outputs": [],
   "source": [
    "X_test_token = pad_sequences(X_test_token, padding='post', maxlen=sequence_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-Wie2nWcLd0"
   },
   "source": [
    "# Word2vec Model + Embedding Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIB_tun5HAfI"
   },
   "outputs": [],
   "source": [
    "train_sents = []\n",
    "for i in range(len(X_train)):\n",
    "  train_sents_intr = SpaceTokenizer().tokenize(X_train[i])\n",
    "  train_sents.append(train_sents_intr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "mRrcKCBOGlDG",
    "outputId": "a229a3b7-4fac-48ef-c792-99237b85d03f"
   },
   "outputs": [],
   "source": [
    "model_w2v_xtrain = Word2Vec(sentences=train_sents, size=100, sg=1, hs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NW8OcLDvqLYY"
   },
   "outputs": [],
   "source": [
    "embedding_matrix_word2vec = np.zeros((vocab_size, 100))\n",
    "for index,word in enumerate(tokenizer.word_index):\n",
    "  try :\n",
    "    #model_w2v.wv[word]\n",
    "    idx = tokenizer.word_index.get(word)\n",
    "    embedding_matrix_word2vec[idx] = np.array(model_w2v.wv[word], dtype=np.float32)[:100]\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHICCT91cZBx"
   },
   "source": [
    "# Fast Text + Embedding Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCiZa4HHcej0"
   },
   "outputs": [],
   "source": [
    "train_sents = []\n",
    "for i in range(len(X_train)):\n",
    "  train_sents_intr = SpaceTokenizer().tokenize(X_train[i])\n",
    "  train_sents.append(train_sents_intr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElViCYkxcoD_"
   },
   "outputs": [],
   "source": [
    "model_fstxt = FastText(train_sents, size=100, window=5, min_count=5, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRgpoGU1qGtY"
   },
   "outputs": [],
   "source": [
    "embedding_matrix_fast_text = np.zeros((vocab_size, 100))\n",
    "for index,word in enumerate(tokenizer.word_index):\n",
    "  try :\n",
    "    idx = tokenizer.word_index.get(word)\n",
    "    embedding_matrix_fast_text[idx] = np.array(model_fstxt.wv[word], dtype=np.float32)[:100]\n",
    "  except:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxROGNqVdn9f"
   },
   "source": [
    "# CNN with keras tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohmUbRlp_BCm",
    "outputId": "703614d7-837c-4f62-950e-9c3f5003e687"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "droprate=0.25\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_len))\n",
    "model.add(layers.Conv1D(filters=128, kernel_size=5, activation='relu',kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(Dropout(droprate))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(droprate))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "lcPVLlZo_dsc",
    "outputId": "70bd92a3-91d9-4b53-e0a6-3da463305535"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_token, y_train, epochs=7, validation_data=(X_val_token, y_val), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljfU0E6Od7Rp"
   },
   "source": [
    "# CNN Word2Vec + Fast Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Hkxi5C7eBVU"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "droprate=0.25\n",
    "model = Sequential()\n",
    "# Modifier le paramètre weights pour changer de word2vec à fasttext ou inversement\n",
    "model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=sequence_len, weights = [embedding_matrix_fast_text], trainable = True))\n",
    "model.add(layers.Conv1D(filters=128, kernel_size=5, activation='relu',kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(Dropout(droprate))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(droprate))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FnYygbFeSru"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_token, y_train, epochs=7, validation_data=(X_val_token, y_val), batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
