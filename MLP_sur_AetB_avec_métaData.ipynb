{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d609c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "tokenizer = nltk.RegexpTokenizer(r'''\\w'|\\w+|[^\\w\\s]''')\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense,Dropout,Embedding,LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "random.seed(123)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36991f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"A.csv\")\n",
    "df_dev = pd.read_csv(\"A.csv\")\n",
    "df_test = pd.read_csv(\"A.csv\")\n",
    "# pour appliquer MLP sur Les données B\n",
    "# df_train = pd.read_csv(\"B.csv\")\n",
    "# df_dev = pd.read_csv(\"B.csv\")\n",
    "# df_test = pd.read_csv(\"B.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9756f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#création des embeddings W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49873c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comments = df_test['commentaire'].values.tolist()\n",
    "dev_comments = df_dev['commentaire'].values.tolist()\n",
    "train_comments = df_train['commentaire'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d466d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = test_comments+dev_comments+train_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ad426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = []\n",
    "length = len(sentences)\n",
    "for i in range(length):\n",
    "    tokens.append(word_tokenize(sentences[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f3786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(tokens, vector_size=100, min_count=1,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef9fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec_final.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "model_w2v = Word2Vec.load(\"word2vec_final.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2703f2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = [' '.join(item.split()[:100]) for item in df_train['commentaire'].values.tolist() if item]\n",
    "dev_list = [' '.join(item.split()[:100]) for item in df_dev['commentaire'].values.tolist() if item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['commentaire'] = train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6890ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev['commentaire']=dev_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05567af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['commentaire']\n",
    "X_dev = df_dev['commentaire']\n",
    "tokenizer = Tokenizer(num_words=len(list(X_train.unique())))\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "X_train = tokenizer.texts_to_sequences(X_train) \n",
    "X_dev = tokenizer.texts_to_sequences(X_dev)\n",
    "X_train = pad_sequences(X_train, maxlen=100)\n",
    "X_dev = pad_sequences(X_dev, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6140ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        vector = model_w2v.wv[word]\n",
    "    except:\n",
    "        continue\n",
    "        print(i)\n",
    "    embedding_matrix[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d982c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing #pour convertir l'id  de users à des chiffres\n",
    "le = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = df_train['user_id'].values.tolist() + df_dev['user_id'].values.tolist() + df_test['user_id'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0051285",
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "\n",
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aa552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarities = []\n",
    "for i in range(len(df_train)):\n",
    "    blob = tb(df_train['commentaire'][i])\n",
    "    polarities.append(blob.sentiment[0])\n",
    "    \n",
    "polarities_dev = []\n",
    "for i in range(len(df_dev)):\n",
    "    blob = tb(df_dev['commentaire'][i])\n",
    "    polarities_dev.append(blob.sentiment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e33200",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uids=le.transform(df_train['user_id'].values.tolist())\n",
    "dev_uids=le.transform(df_dev['user_id'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ec78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train.tolist()\n",
    "for i in range(len(x_train)): \n",
    "    x_train[i].insert(0,df_train['movie'][i])\n",
    "    x_train[i].insert(0,train_uids[i])\n",
    "    x_train[i].insert(0,polarities[i])\n",
    "x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49015f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dev = X_dev.tolist()\n",
    "for i in range(len(x_dev)): \n",
    "    x_dev[i].insert(0,df_dev['movie'][i])\n",
    "    x_dev[i].insert(0,dev_uids[i])\n",
    "    x_dev[i].insert(0,polarities_dev[i])\n",
    "x_dev = np.array(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab815956",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(df_train['note'])\n",
    "y_dev = np.array(df_dev['note'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b40283",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb45681",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size,\n",
    "                            103,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=100,\n",
    "                            trainable=True))\n",
    "model.add(tf.keras.layers.Flatten(input_shape=(102, 100)))\n",
    "model.add(tf.keras.layers.Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "#create callback\n",
    "filepath = 'my_best_model_MLP_with_meta_data_A.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, \n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1, \n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "\n",
    "history=model.fit(X_train, y_train, validation_data=(X_dev, y_dev),epochs=20, batch_size=128, verbose=1,callbacks=callbacks)\n",
    "\n",
    "elapsed_time = time.time() - st\n",
    "print('Execution time:', time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791225b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = Df_test['commentaire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c61867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('my_best_model_MLP_with_meta_data_A.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(X_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6809ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_uids=le.transform(df_test['user_id'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f648168",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarities_test = []\n",
    "for i in range(len(df_test)):\n",
    "    blob = tb(df_test['commentaire'][i])\n",
    "    polarities_test.append(blob.sentiment[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb7a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.tolist()\n",
    "for i in range(len(X_test)): \n",
    "    X_test[i].insert(0,Df_test['movie'][i])\n",
    "    X_test[i].insert(0,test_uids[i])\n",
    "    X_test[i].insert(0,polarities_test[i])\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536fd7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(yhat,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a5f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = [] #génération du fichier pour la plateforme d'évaluation\n",
    "with open(\"sortie_my_best_model_MLP_with_meta_data_A.txt\", \"w\") as f:\n",
    "    for i in range(len(df_test['review_id'])):\n",
    "        notes.append((prediction[i]+1)/2)\n",
    "        f.write(str(df_test['review_id'].iloc[i])+' '+str(notes[i]).replace('.',',')+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
